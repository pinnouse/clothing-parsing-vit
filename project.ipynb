{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import imageio.v3 as iio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data parsing class: PhotoData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class PhotoData(object):\n",
    "\n",
    "    SRC_FOLDER = 'images/'\n",
    "\n",
    "    LBL_FOLDER = 'labels/'\n",
    "\n",
    "    HDF5_FILE = 'all_images.h5'\n",
    "\n",
    "    N = 600\n",
    "\n",
    "    def indextofilename(index) -> str:\n",
    "        return f\"{index:04d}.jpg\"\n",
    "    \n",
    "    def indextolabelname(index) -> Tuple[str]:\n",
    "        ind = f\"{index:04d}\"\n",
    "        return f\"{ind}_person.png\", f\"{ind}_clothes.png\"\n",
    "\n",
    "\n",
    "    def loadimages(self):\n",
    "        images = np.ndarray((PhotoData.N, 600, 400, 3))\n",
    "        for i in range(PhotoData.N):\n",
    "            img_url = os.path.join(PhotoData.SRC_FOLDER, PhotoData.indextofilename(i + 1))\n",
    "            im = iio.imread(img_url)\n",
    "            images[i] = im\n",
    "        return images\n",
    "    \n",
    "    def loadlabels(self):\n",
    "        p_labels = np.ndarray((PhotoData.N, 600, 400), dtype=bool)\n",
    "        c_labels = np.ndarray((PhotoData.N, 600, 400), dtype=np.int8)\n",
    "\n",
    "        for i in range(PhotoData.N):\n",
    "            p_name, c_name = PhotoData.indextolabelname(i+1)\n",
    "            p_url = os.path.join(PhotoData.LBL_FOLDER, p_name)\n",
    "            c_url = os.path.join(PhotoData.LBL_FOLDER, c_name)\n",
    "            p_im = iio.imread(p_url)\n",
    "            c_im = iio.imread(c_url)\n",
    "            p_labels[i] = p_im[:,:,0] > 0\n",
    "            # https://stackoverflow.com/questions/15635025/how-to-map-false-color-image-to-specific-labels-assigned-for-each-color\n",
    "            r_channel = c_im[:, :, 0] > 0\n",
    "            g_channel = c_im[:, :, 1] > 0\n",
    "            b_channel = c_im[:, :, 2] > 0\n",
    "            labels = (b_channel << 2) + (g_channel << 1) + r_channel\n",
    "            c_labels[i] = labels\n",
    "        return p_labels, c_labels\n",
    "    \n",
    "    def parsedata(self):\n",
    "        images = self.loadimages()\n",
    "        p_labels, c_labels = self.loadlabels()\n",
    "        return images, p_labels, c_labels\n",
    "\n",
    "    def storeh5(self, images, p_labels, c_labels):\n",
    "        file = h5py.File(PhotoData.HDF5_FILE, 'w')\n",
    "        file.create_dataset('images', np.shape(images), h5py.h5t.STD_U8BE, data=images)\n",
    "        file.create_dataset('person_labels', np.shape(p_labels), h5py.h5t.STD_U8BE, data=p_labels)\n",
    "        file.create_dataset('clothing_labels', np.shape(c_labels), h5py.h5t.STD_U8BE, data=c_labels)\n",
    "        file.close()\n",
    "\n",
    "    def loadh5(self):\n",
    "        images, p_labels, c_labels = [], [], []\n",
    "        file = h5py.File(PhotoData.HDF5_FILE, 'r+')\n",
    "        images = np.array(file['images']).astype('uint8')\n",
    "        p_labels = np.array(file['person_labels']).astype('float32')\n",
    "        c_labels = np.array(file['clothing_labels']).astype('float32')\n",
    "        return images, p_labels, c_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process and Store data\n",
    "parse and store the data in h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "data = PhotoData()\n",
    "i, p, c = data.parsedata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subdivide images for ViT\n",
    "def subdivideimage(imgs: np.ndarray, suppixsize=16):\n",
    "    \"\"\"Pads the images and subdivides for number of pixels required\"\"\"\n",
    "    N = imgs.shape[0]\n",
    "    H = imgs.shape[1]\n",
    "    W = imgs.shape[2]\n",
    "    rows = math.ceil(H / suppixsize)\n",
    "    # cols = math.ceil(W / suppixsize)\n",
    "\n",
    "    has_color = len(imgs.shape) > 3\n",
    "\n",
    "    dims = (N, 1, W, 3) if has_color else (N, 1, W,)\n",
    "    if H % suppixsize > 0:\n",
    "        for i in range(suppixsize - (H % suppixsize)):\n",
    "            imgs = np.append(imgs, np.zeros(dims), axis=1)\n",
    "    new_H = rows * suppixsize\n",
    "    dims = (N, new_H, 1, 3) if has_color else (N, new_H, 1,)\n",
    "    if W % suppixsize > 0:\n",
    "        for i in range(suppixsize - (W % suppixsize)):\n",
    "            imgs = np.append(imgs, np.zeros(dims), axis=2)\n",
    "    # return imgs\n",
    "    # https://stackoverflow.com/questions/16856788/slice-2d-array-into-smaller-2d-arrays\n",
    "    swap_dim_1 = [N, rows, suppixsize, -1, suppixsize]\n",
    "    swap_dim_2 = [N, -1, suppixsize, suppixsize]\n",
    "    if has_color:\n",
    "        swap_dim_1.append(3)\n",
    "        swap_dim_2.append(3)\n",
    "    return imgs.reshape(swap_dim_1).swapaxes(2,3).reshape(swap_dim_2)\n",
    "\n",
    "images = subdivideimage(i)\n",
    "p_labels = subdivideimage(p)\n",
    "c_labels = subdivideimage(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape images\n",
    "def reshapeimages(images):\n",
    "    N = images.shape[0]\n",
    "    M = images.shape[1]\n",
    "    return images.reshape((N, M, -1))\n",
    "\n",
    "images = reshapeimages(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# produce [2] labels\n",
    "def processpersonlabels(p_labels):\n",
    "    N = p_labels.shape[0]\n",
    "    M = p_labels.shape[1]\n",
    "    total = p_labels.shape[2] ** 2\n",
    "    person_arr = (np.sum(p_labels.reshape((N, M, -1)), axis=2, dtype=float) / total).reshape((N, M, 1))\n",
    "    background_arr = np.ones((N, M, 1)) - person_arr\n",
    "    return np.append(background_arr, person_arr, axis=2)\n",
    "pp = processpersonlabels(p_labels=p_labels)\n",
    "pp[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# produce [7] labels\n",
    "def processclotheslabels(c_labels):\n",
    "    N = c_labels.shape[0]\n",
    "    M = c_labels.shape[1]\n",
    "    total = c_labels.shape[2] ** 2\n",
    "    flattened = c_labels.reshape((N, M, -1))\n",
    "    cc = np.zeros((N, M, 0))\n",
    "    for i in range(7):\n",
    "        x = (np.sum(flattened == i, axis=2, dtype=float) / total).reshape((N, M, 1))\n",
    "        cc = np.append(cc, x, axis=2)\n",
    "    return cc\n",
    "\n",
    "cc = processclotheslabels(c_labels)\n",
    "cc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.storeh5(images, pp, cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PhotoData()\n",
    "images, p_labels, c_labels = data.loadh5()\n",
    "p_labels = p_labels.swapaxes(1,2)\n",
    "c_labels = c_labels.swapaxes(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 988, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Define the model, will be writing a vision transformer (ViT, https://arxiv.org/abs/2010.11929v2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/school/csc420/final/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# default: d_model=768, nhead=12, num_layers=12\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=768, nhead=8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6).to(device)\n",
    "# src = torch.rand(3, 968, 768, device=device)\n",
    "# out = transformer_encoder(src) # torch.Size([3, 968, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_model = nn.Sequential(\n",
    "    transformer_encoder,\n",
    "    nn.Linear(768, 2),\n",
    "    nn.Softmax(dim=2)\n",
    ").to(device)\n",
    "\n",
    "c_model = nn.Sequential(\n",
    "    transformer_encoder,\n",
    "    nn.Linear(768, 7),\n",
    "    nn.Softmax(dim=2)\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "p_optimizer = optim.AdamW(p_model.parameters())\n",
    "c_optimizer = optim.AdamW(c_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "bce = nn.BCELoss().to(device)\n",
    "cel = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "def train(model, src, tgt, optimizer, loss_fn=cel, ckpt_name=None, batch_size=64, epochs=3):\n",
    "    N = src.shape[0]\n",
    "    losses = []\n",
    "    batches = N // batch_size\n",
    "    model.train()\n",
    "    \n",
    "    print(f\"Starting training: {batches} batches per epoch for {epochs} epochs\")\n",
    "    for e in range(epochs):\n",
    "        print(f\"\\tStarting epoch {e+1}\")\n",
    "        starttime = time.time()\n",
    "        losses.append([])\n",
    "        for i in range(batches):\n",
    "            start = i * batch_size\n",
    "            train_src = src[start:start + batch_size]\n",
    "            train_tgt = tgt[start:start + batch_size]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(train_src).swapaxes(1, 2)\n",
    "            loss = loss_fn(output, train_tgt)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses[e].append(loss.item())\n",
    "        \n",
    "        print(f\"\\tEpoch {e+1:02d} took {time.time() - starttime}s.\\n\\tTotal loss at end of epoch: {sum(losses[e]) / batches}\")\n",
    "\n",
    "        if ckpt_name is not None:\n",
    "            torch.save({\n",
    "                'epoch': e+1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'batch_losses': losses[e]\n",
    "            }, f\"checkpoints/{ckpt_name}-epoch{e+1}.pt\")\n",
    "\n",
    "    return losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 300\n",
    "src = torch.tensor(images[:train_size], dtype=torch.float32).to(device)\n",
    "tgt_p = torch.tensor(p_labels[:train_size], dtype=torch.float32).to(device)\n",
    "tgt_c = torch.tensor(c_labels[:train_size], dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training: 18 batches per epoch for 15 epochs\n",
      "\tStarting epoch 1\n",
      "\tEpoch 01 took 288.2376284599304s.\n",
      "\tTotal loss at end of epoch: 1.5516875916057162\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory checkpoints does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/nick/school/csc420/final/project.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nick/school/csc420/final/project.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train person identifier\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/nick/school/csc420/final/project.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m p_losses \u001b[39m=\u001b[39m train(p_model, src, tgt_p, p_optimizer, loss_fn\u001b[39m=\u001b[39;49mbce, ckpt_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mperson_model\u001b[39;49m\u001b[39m'\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m)\n",
      "\u001b[1;32m/home/nick/school/csc420/final/project.ipynb Cell 20\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nick/school/csc420/final/project.ipynb#X25sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m02d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m took \u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m.\u001b[39mtime()\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mstarttime\u001b[39m}\u001b[39;00m\u001b[39ms.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mTotal loss at end of epoch: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39msum\u001b[39m(losses[e])\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39mbatches\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nick/school/csc420/final/project.ipynb#X25sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39mif\u001b[39;00m ckpt_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/nick/school/csc420/final/project.ipynb#X25sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m         torch\u001b[39m.\u001b[39;49msave({\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nick/school/csc420/final/project.ipynb#X25sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39mepoch\u001b[39;49m\u001b[39m'\u001b[39;49m: e\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nick/school/csc420/final/project.ipynb#X25sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39mmodel_state_dict\u001b[39;49m\u001b[39m'\u001b[39;49m: model\u001b[39m.\u001b[39;49mstate_dict(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nick/school/csc420/final/project.ipynb#X25sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39moptimizer_state_dict\u001b[39;49m\u001b[39m'\u001b[39;49m: optimizer\u001b[39m.\u001b[39;49mstate_dict(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nick/school/csc420/final/project.ipynb#X25sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39mbatch_losses\u001b[39;49m\u001b[39m'\u001b[39;49m: losses[e]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nick/school/csc420/final/project.ipynb#X25sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         }, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcheckpoints/\u001b[39;49m\u001b[39m{\u001b[39;49;00mckpt_name\u001b[39m}\u001b[39;49;00m\u001b[39m-epoch\u001b[39;49m\u001b[39m{\u001b[39;49;00me\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nick/school/csc420/final/project.ipynb#X25sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mreturn\u001b[39;00m losses\n",
      "File \u001b[0;32m~/school/csc420/final/.venv/lib/python3.11/site-packages/torch/serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    625\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 628\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    629\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    630\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/school/csc420/final/.venv/lib/python3.11/site-packages/torch/serialization.py:502\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m     container \u001b[39m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 502\u001b[0m \u001b[39mreturn\u001b[39;00m container(name_or_buffer)\n",
      "File \u001b[0;32m~/school/csc420/final/.venv/lib/python3.11/site-packages/torch/serialization.py:473\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39mPyTorchFileWriter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_stream))\n\u001b[1;32m    472\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 473\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mPyTorchFileWriter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory checkpoints does not exist."
     ]
    }
   ],
   "source": [
    "# Train person identifier\n",
    "p_losses = train(p_model, src, tgt_p, p_optimizer, loss_fn=bce, ckpt_name='person_model', batch_size=16, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training: 18 batches per epoch for 6 epochs\n",
      "\tStarting epoch 1\n",
      "\tTotal loss at end of epoch 1: 1.646254645453559\n",
      "\tStarting epoch 2\n",
      "\tTotal loss at end of epoch 2: 1.6462497446272109\n",
      "\tStarting epoch 3\n",
      "\tTotal loss at end of epoch 3: 1.646322210629781\n",
      "\tStarting epoch 4\n",
      "\tTotal loss at end of epoch 4: 1.646307733323839\n",
      "\tStarting epoch 5\n",
      "\tTotal loss at end of epoch 5: 1.6462871829668682\n",
      "\tStarting epoch 6\n",
      "\tTotal loss at end of epoch 6: 1.6462660895453558\n"
     ]
    }
   ],
   "source": [
    "# Train clothing identifier\n",
    "c_losses = train(c_model, src, tgt_c, c_optimizer, ckpt_name='clothes_model', batch_size=16, epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, valid_src, valid_tgt, loss_fn):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(valid_src).swapaxes(1, 2)\n",
    "        loss = loss_fn(output, valid_tgt)\n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_size = 60\n",
    "valid_src = torch.tensor(images[train_size:train_size + validation_size], dtype=torch.float32).to(device)\n",
    "valid_tgt_p = torch.tensor(p_labels[train_size:train_size + validation_size], dtype=torch.float32).to(device)\n",
    "valid_tgt_c = torch.tensor(c_labels[train_size:train_size + validation_size], dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.817803144454956"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(p_model, valid_src, valid_tgt_p, bce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and infer with Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('person_model-epoch6.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "p_model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
